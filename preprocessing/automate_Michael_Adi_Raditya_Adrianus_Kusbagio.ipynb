{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "vcEoJHqeut93"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "import requests\n",
        "import csv\n",
        "from io import StringIO, BytesIO\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "class TextPreprocessor:\n",
        "    def __init__(self):\n",
        "        self.stop_words = self._get_stopwords()\n",
        "        self.lexicon_positive, self.lexicon_negative = self._get_lexicons()\n",
        "        self.kamus_slang = self._load_remote_slang()\n",
        "\n",
        "    def _get_stopwords(self):\n",
        "        import nltk\n",
        "        nltk.download('punkt')\n",
        "        nltk.download('stopwords')\n",
        "        nltk.download('punkt_tab')\n",
        "        stop_words = set(stopwords.words('indonesian'))\n",
        "        stop_words.update(set(stopwords.words('english')))\n",
        "        return stop_words\n",
        "\n",
        "    def _load_remote_slang(self):\n",
        "        url = \"https://github.com/MichaelAdi434/Project-Analisis-Sentimen/raw/d21c7566deca33e2871f160f19728f39d5fd273d/kamuskatabaku.xlsx\"\n",
        "        try:\n",
        "            response = requests.get(url)\n",
        "            if response.status_code == 200:\n",
        "                df_slang = pd.read_excel(BytesIO(response.content))\n",
        "                return dict(zip(df_slang.iloc[:, 0], df_slang.iloc[:, 1]))\n",
        "            else:\n",
        "                print(\"Gagal mengambil kamus dari GitHub, menggunakan kamus kosong.\")\n",
        "                return {}\n",
        "        except Exception as e:\n",
        "            print(f\"Error saat memuat kamus: {e}\")\n",
        "            return {}\n",
        "\n",
        "    def _get_lexicons(self):\n",
        "        pos = {}\n",
        "        neg = {}\n",
        "        # Load Positive Lexicon\n",
        "        res_pos = requests.get('https://raw.githubusercontent.com/angelmetanosaa/dataset/main/lexicon_positive.csv')\n",
        "        if res_pos.status_code == 200:\n",
        "            reader = csv.reader(StringIO(res_pos.text))\n",
        "            pos = {row[0]: int(row[1]) for row in reader}\n",
        "\n",
        "        # Load Negative Lexicon\n",
        "        res_neg = requests.get('https://raw.githubusercontent.com/angelmetanosaa/dataset/main/lexicon_negative.csv')\n",
        "        if res_neg.status_code == 200:\n",
        "            reader = csv.reader(StringIO(res_neg.text))\n",
        "            neg = {row[0]: int(row[1]) for row in reader}\n",
        "\n",
        "        return pos, neg\n",
        "\n",
        "    def clean_text(self, text):\n",
        "        text = str(text)\n",
        "        text = re.sub(r'@[A-Za-z0-9]+|#[A-Za-z0-9]+|RT[\\s]|http\\S+|[0-9]+', '', text)\n",
        "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "        text = text.lower().strip()\n",
        "        return text\n",
        "\n",
        "    def handle_slang(self, text):\n",
        "        return ' '.join([self.kamus_slang.get(kata, kata) for kata in text.split()])\n",
        "\n",
        "    def process_tokens(self, text):\n",
        "        tokens = word_tokenize(text)\n",
        "        return [t for t in tokens if t not in self.stop_words]\n",
        "\n",
        "    def get_sentiment(self, tokens):\n",
        "        score = sum(self.lexicon_positive.get(t, 0) for t in tokens)\n",
        "        score += sum(self.lexicon_negative.get(t, 0) for t in tokens)\n",
        "        return 'positive' if score >= 0 else 'negative'\n",
        "\n",
        "    def run_pipeline(self, df, text_column='content'):\n",
        "\n",
        "        df = df.dropna(subset=[text_column]).drop_duplicates()\n",
        "        df['clean_text'] = df[text_column].apply(self.clean_text)\n",
        "        df['normalized'] = df['clean_text'].apply(self.handle_slang)\n",
        "        df['tokens'] = df['normalized'].apply(self.process_tokens)\n",
        "        df['text_akhir'] = df['tokens'].apply(lambda x: ' '.join(x))\n",
        "        df['label'] = df['tokens'].apply(self.get_sentiment)\n",
        "\n",
        "        print(\"Preprocessing selesai!\")\n",
        "        return df[['text_akhir', 'label']]\n",
        "\n"
      ]
    }
  ]
}